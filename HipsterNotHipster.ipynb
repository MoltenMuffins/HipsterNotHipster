{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HipsterNotHipster.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MoltenMuffins/HipsterNotHipster/blob/master/HipsterNotHipster.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "XH2e3hR8lZrL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Hipster/NotHipster\n",
        "\n",
        "### Problem Statement\n",
        "This project attempts to create one class image classifier via transfer learning on top of VGG-Face as a final project submission for the December iteration of the Deep Learning Jump Start Workshop conducted by Red Dragon AI. The dataset used is self generated using a bulk image downloading tool (fatkun) while the VGG-Face weights for tensorflow are kindly provided by Sefik at https://sefiks.com/2018/08/06/deep-face-recognition-with-keras/. While the choice of topic \"Hipster vs Not Hipster\" seems rather whimsical, my intention with the project was to try to captilize on the ability of neural networks to generate features independently and see if it can replicate subjective human categorization.\n",
        "\n",
        "### Possible Issues and Actions Taken\n",
        "*   The prevalence of beards in contemporary hipster culture leads to many of the 'hipster' positive examples showing beards. My attempt to mitigate this involves including sufficient examples of non-hipsters with beards (eg. muslim men). \n",
        "*   Confirmation bias is also present as the training examples are created by me and hence the classifier would be directly affect by my own perceptions of what constitutes a hipster. The accuracy of the model in use thus varies from person to person.\n",
        "*  I did not have a rubric for classifying hipsters and allocated labels to images solely based on my own gut feeling which could cause inconsistency in the labeling.\n",
        "* Due to my fixation on beards, I neglected to include examples of the female gender in this run of the project. A third iteration of this project would include the female gender, or better yet include a spectrum of genders.\n",
        "\n",
        "### Summary of Results\n",
        "My initial hypothesis would be that the imagenet model with weights would perform favorably as I felt the clothes and accessories on the person would be a key component of identifying a hipster. An earlier iteration of this project involved using the standard VGG-16 model with imagenet weights, and produced results around *~0.50*.\n",
        "\n",
        "My second approach is contained in this notebook. It uses the VGGface model which essentially is a VGG model trained on celebrity faces. The process of implementing this model took longer (mainly due to time spent trying to understand the VGGface topology and finding a source for VGGface weights as the relevant github repo has outdated code that can't be run) but produces slightly better results of *~0.67*. \n",
        "\n",
        "This current build seems to work well and suggests that whether one is a hipster has more to do with one's facial features and hair rather than their dressing. I expect that this model could exhibit improvements in performance if given a larger dataset. For a third iteration, a larger dataset would definitely be required. In order to increase the efficiency of labeling, I might look into builing an application or tool modeled after tinder where one could simply swipe left or right to label images. This would greatly speed up the dataset creation process for this project and any other future endeavors.\n"
      ]
    },
    {
      "metadata": {
        "id": "uxfBGXnEoYJm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 0. Testing for GPU"
      ]
    },
    {
      "metadata": {
        "id": "qb_AGVrBofk8",
        "colab_type": "code",
        "outputId": "c2326f50-9a9f-4e9e-af8e-b9bc1a394a63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_fWPXMbrW6md",
        "colab_type": "code",
        "outputId": "9676008b-61f8-4aaf-876c-86a2d8e8393a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#Boilerplate code\n",
        "import os, json\n",
        "from glob import glob\n",
        "\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from tensorflow.keras.models import Model,load_model,Sequential\n",
        "\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.VERSION"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.12.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "metadata": {
        "id": "dkRq-Jo3EiTk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Download VGGface weights. These are orignially weights for caffe and converted to \n",
        "# keras and obtained from https://sefiks.com/2018/08/06/deep-face-recognition-with-keras/\n",
        "!wget -qq https://www.dropbox.com/s/lhmr5sa6lcr0f23/vgg_face_weights.h5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qVYgtJODcomK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##  1. Dataset Download and Organization\n",
        "\n",
        "We use a small dataset that I prepared using FatKun which has been pre-labeled. \n",
        "\n",
        "It has the following file structure: \n",
        "`Data/hipsternothipster/{CLASS_LABEL}/{FILENAME}.jpg`"
      ]
    },
    {
      "metadata": {
        "id": "CktJpSO2-gGD",
        "colab_type": "code",
        "outputId": "2f60caa9-2c64-4981-d502-3058c505b92f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "#Download dataset\n",
        "\n",
        "!wget -qq https://www.dropbox.com/s/cqy2r8xmnqfoh5n/data.zip\n",
        "!unzip -qq data.zip\n",
        "!rm data.zip\n",
        "!rm -r __MACOSX\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "replace data/.DS_Store? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n",
            "data  sample_data  vgg_face_weights.h5\tvgg_face_weights.h5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PdOyJEzupBxq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2. Preparing Dataset for our model\n",
        "\n",
        "\n",
        "\n",
        "We use glob to get lists of the files in the directories and then convert them into dataframes and add in class numbers.\n",
        "\n",
        "We also then split them up so we have 10% for a testing set and the rest for training.\n",
        "\n",
        "Finally we randomly shuffle them up\n"
      ]
    },
    {
      "metadata": {
        "id": "8a8MtVhkX7DA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "path = \"./data/hipsternothipster/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KLXYXYXaYE7P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_data_dir = \"./data/hipsternothipster/train/\"\n",
        "validation_data_dir = \"./data/hipsternothipster/valid/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qZ8dtbLKYIZC",
        "colab_type": "code",
        "outputId": "8a160f75-91bd-480b-bfcf-7f0b5bd16bfe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import pandas as pd\n",
        "\n",
        "filenames_n0 = glob.glob('./data/hipsternothipster/train/nothipster/*.jpg')\n",
        "filenames_n1 = glob.glob('./data/hipsternothipster/train/hipster/*.jpg')\n",
        "\n",
        "\n",
        "names = ['nothipster', 'hipster']\n",
        "\n",
        "\n",
        "len(filenames_n1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "186"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "metadata": {
        "id": "DXnbSIuGYLnT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Make a dataframe based on the filenames\n",
        "df = pd.DataFrame(filenames_n0, columns = [\"filename\"])\n",
        "df2 = pd.DataFrame(filenames_n1, columns = [\"filename\"])\n",
        "\n",
        "\n",
        "# Add Class columns \n",
        "df['class'] = pd.Series([0 for x in range(len(df.index))], index=df.index)\n",
        "df2['class'] = pd.Series([1 for x in range(len(df2.index))], index=df2.index)\n",
        "\n",
        "\n",
        "\n",
        "# Split into train and validation sets\n",
        "train_set_percentage = .9\n",
        "\n",
        "\n",
        "train_df = df[:int(len(df)*train_set_percentage)]\n",
        "val_df = df[int(len(df)*train_set_percentage):]\n",
        "\n",
        "train_df2 = df2[:int(len(df2)*train_set_percentage)]\n",
        "val_df2 = df2[int(len(df2)*train_set_percentage):]\n",
        "\n",
        "\n",
        "df_new_train = pd.concat([train_df, train_df2])\n",
        "df_new_val = pd.concat([val_df, val_df2])\n",
        "\n",
        "# shuffle \n",
        "df = df_new_train.sample(frac=1).reset_index(drop=True)\n",
        "df_val = df_new_val.sample(frac=1).reset_index(drop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u1RzNm8OYNaI",
        "colab_type": "code",
        "outputId": "76593da4-30e0-4d97-8b38-d24a8ab16ba5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>./data/hipsternothipster/train/hipster/hipster...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>./data/hipsternothipster/train/hipster/hipster...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>./data/hipsternothipster/train/hipster/hipster...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>./data/hipsternothipster/train/nothipster/noth...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>./data/hipsternothipster/train/hipster/hipster...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            filename  class\n",
              "0  ./data/hipsternothipster/train/hipster/hipster...      1\n",
              "1  ./data/hipsternothipster/train/hipster/hipster...      1\n",
              "2  ./data/hipsternothipster/train/hipster/hipster...      1\n",
              "3  ./data/hipsternothipster/train/nothipster/noth...      0\n",
              "4  ./data/hipsternothipster/train/hipster/hipster...      1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "metadata": {
        "id": "bjLPwcLDYPNd",
        "colab_type": "code",
        "outputId": "ae5d1815-4cfd-4898-873a-0c90c5ad8aed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# convert the dataframe into 2 lists to use for filename and labels\n",
        "train_filenames_list = df[\"filename\"].tolist()\n",
        "train_labels_list = df[\"class\"].astype('int32').tolist()\n",
        "\n",
        "# convert the dataframe into 2 lists to use for filename and labels\n",
        "val_filenames_list = df_val[\"filename\"].tolist()\n",
        "val_labels_list = df_val[\"class\"].astype('int32').tolist()\n",
        "\n",
        "#number of classes\n",
        "num_classes = 2\n",
        "\n",
        "df.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(328, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "metadata": {
        "id": "tMjABvXyYQuI",
        "colab_type": "code",
        "outputId": "0fa71851-68e9-4bcf-cf01-b8d804ddf2f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "train_filenames_list[:5]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['./data/hipsternothipster/train/hipster/hipster.169.jpg',\n",
              " './data/hipsternothipster/train/hipster/hipster.127.jpg',\n",
              " './data/hipsternothipster/train/hipster/hipster.072.jpg',\n",
              " './data/hipsternothipster/train/nothipster/nothipster.062.jpg',\n",
              " './data/hipsternothipster/train/hipster/hipster.018.jpg']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "metadata": {
        "id": "MCez2vyp7D__",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3. Make the pipeline for loading and resizing the images\n"
      ]
    },
    {
      "metadata": {
        "id": "RPFnQLzC7F2c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Reads an image from a file, decodes it into a tensor, and resizes it\n",
        "# to a fixed shape.\n",
        "img_rows, img_cols = 224,224\n",
        "\n",
        "def _parse_function(filename, label):\n",
        "  image_string = tf.read_file(filename)\n",
        "  #Channels specified to be 3 to ensure images output as [224,224,3] at the end of this\n",
        "  image_decoded = tf.image.decode_jpeg(image_string,channels=3)\n",
        "  image_resized = tf.image.resize_images(image_decoded, [img_rows, img_cols])\n",
        "  label = tf.one_hot(label, num_classes)\n",
        "  return image_resized, label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NN29pKF17I7W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create vector of filenames from list\n",
        "filenames = tf.constant(train_filenames_list)\n",
        "\n",
        "# Create vector of labels\n",
        "labels = tf.constant(train_labels_list)\n",
        "\n",
        "# Same as above but for validation set\n",
        "val_filenames = tf.constant(val_filenames_list)\n",
        "val_labels = tf.constant(val_labels_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IDcXMTc37Lp8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 4. Assembling the Data pipeline using tf.data\n"
      ]
    },
    {
      "metadata": {
        "id": "d466t0z9G1Ow",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
        "train_dataset = train_dataset.map(_parse_function)\n",
        "train_dataset = train_dataset.repeat()\n",
        "train_dataset = train_dataset.batch(32)\n",
        "\n",
        "valid_dataset = tf.data.Dataset.from_tensor_slices((val_filenames, val_labels))\n",
        "valid_dataset = valid_dataset.map(_parse_function)\n",
        "valid_dataset = valid_dataset.repeat()\n",
        "valid_dataset = valid_dataset.batch(32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CPkuSWQ3jmcn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 5. Loading Trained Model Weights\n",
        "\n",
        "We utilize a pretrained model, VGGface, for which we intend to utilize Transfer Learning on. This allows us to leverage on the compute put into the pretrained model, reducing the need for training on our end."
      ]
    },
    {
      "metadata": {
        "id": "yl30IrKhQwh3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from keras.models import model_from_json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "08u2gZn5RmXY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#VGG-Face model is slightly different from VGG-16. Fortunately, the blogger who provided the weights\n",
        "#Earlier also transcribed the model for us link -> https://sefiks.com/2018/08/06/deep-face-recognition-with-keras/\n",
        "base_model = Sequential()\n",
        "\n",
        "base_model.add(ZeroPadding2D((1,1),input_shape=(224,224, 3)))\n",
        "base_model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
        "base_model.add(ZeroPadding2D((1,1)))\n",
        "base_model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
        "base_model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
        " \n",
        "base_model.add(ZeroPadding2D((1,1)))\n",
        "base_model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
        "base_model.add(ZeroPadding2D((1,1)))\n",
        "base_model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
        "base_model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
        " \n",
        "base_model.add(ZeroPadding2D((1,1)))\n",
        "base_model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
        "base_model.add(ZeroPadding2D((1,1)))\n",
        "base_model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
        "base_model.add(ZeroPadding2D((1,1)))\n",
        "base_model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
        "base_model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
        " \n",
        "base_model.add(ZeroPadding2D((1,1)))\n",
        "base_model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
        "base_model.add(ZeroPadding2D((1,1)))\n",
        "base_model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
        "base_model.add(ZeroPadding2D((1,1)))\n",
        "base_model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
        "base_model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
        " \n",
        "base_model.add(ZeroPadding2D((1,1)))\n",
        "base_model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
        "base_model.add(ZeroPadding2D((1,1)))\n",
        "base_model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
        "base_model.add(ZeroPadding2D((1,1)))\n",
        "base_model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
        "base_model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
        " \n",
        "base_model.add(Convolution2D(4096, (7, 7), activation='relu'))\n",
        "base_model.add(Dropout(0.5))\n",
        "base_model.add(Convolution2D(4096, (1, 1), activation='relu'))\n",
        "base_model.add(Dropout(0.5))\n",
        "base_model.add(Convolution2D(2622, (1, 1)))\n",
        "base_model.add(Flatten())\n",
        "base_model.add(Activation('softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BXebJyxfYZk9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Loading the weights\n",
        "base_model.load_weights('vgg_face_weights.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Mga083CqYdwa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# The rest is standard transfer learning\n",
        "\n",
        "# Here we add a global spatial average pooling layer\n",
        "x = base_model.output\n",
        "\n",
        "x = Flatten()(x)\n",
        "\n",
        "# Add a fully-connected layer\n",
        "x = Dense(512, activation='relu')(x)\n",
        "\n",
        "# Add logits layer for 2 classes\n",
        "predictions = Dense(2, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ds8NuYeoYnxe",
        "colab_type": "code",
        "outputId": "459d112e-d800-4cd2-d29c-001f0ea278fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        }
      },
      "cell_type": "code",
      "source": [
        "# Train only the top layers which were randomly initalized and freeze all convolutional VGG16 layers\n",
        "for layer in base_model.layers:\n",
        "    print(layer.name)\n",
        "    layer.trainable = False"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "zero_padding2d_13\n",
            "conv2d_16\n",
            "zero_padding2d_14\n",
            "conv2d_17\n",
            "max_pooling2d_5\n",
            "zero_padding2d_15\n",
            "conv2d_18\n",
            "zero_padding2d_16\n",
            "conv2d_19\n",
            "max_pooling2d_6\n",
            "zero_padding2d_17\n",
            "conv2d_20\n",
            "zero_padding2d_18\n",
            "conv2d_21\n",
            "zero_padding2d_19\n",
            "conv2d_22\n",
            "max_pooling2d_7\n",
            "zero_padding2d_20\n",
            "conv2d_23\n",
            "zero_padding2d_21\n",
            "conv2d_24\n",
            "zero_padding2d_22\n",
            "conv2d_25\n",
            "max_pooling2d_8\n",
            "zero_padding2d_23\n",
            "conv2d_26\n",
            "zero_padding2d_24\n",
            "conv2d_27\n",
            "zero_padding2d_25\n",
            "conv2d_28\n",
            "max_pooling2d_9\n",
            "conv2d_29\n",
            "dropout_2\n",
            "conv2d_30\n",
            "dropout_3\n",
            "conv2d_31\n",
            "flatten_2\n",
            "activation_1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "puh2QGQnYrj7",
        "colab_type": "code",
        "outputId": "efa57fc3-a780-407a-f276-4f85a8020892",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1584
        }
      },
      "cell_type": "code",
      "source": [
        "model.summary()\n",
        "#incoming wall of text, brace yourself"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "zero_padding2d_13_input (Inp (None, 224, 224, 3)       0         \n",
            "_________________________________________________________________\n",
            "zero_padding2d_13 (ZeroPaddi (None, 226, 226, 3)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_16 (Conv2D)           (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "zero_padding2d_14 (ZeroPaddi (None, 226, 226, 64)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_17 (Conv2D)           (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "zero_padding2d_15 (ZeroPaddi (None, 114, 114, 64)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_18 (Conv2D)           (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "zero_padding2d_16 (ZeroPaddi (None, 114, 114, 128)     0         \n",
            "_________________________________________________________________\n",
            "conv2d_19 (Conv2D)           (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "zero_padding2d_17 (ZeroPaddi (None, 58, 58, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_20 (Conv2D)           (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "zero_padding2d_18 (ZeroPaddi (None, 58, 58, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_21 (Conv2D)           (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "zero_padding2d_19 (ZeroPaddi (None, 58, 58, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_22 (Conv2D)           (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "zero_padding2d_20 (ZeroPaddi (None, 30, 30, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_23 (Conv2D)           (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "zero_padding2d_21 (ZeroPaddi (None, 30, 30, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_24 (Conv2D)           (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "zero_padding2d_22 (ZeroPaddi (None, 30, 30, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_25 (Conv2D)           (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "zero_padding2d_23 (ZeroPaddi (None, 16, 16, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_26 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "zero_padding2d_24 (ZeroPaddi (None, 16, 16, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_27 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "zero_padding2d_25 (ZeroPaddi (None, 16, 16, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_28 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2 (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_29 (Conv2D)           (None, 1, 1, 4096)        102764544 \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 1, 1, 4096)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_30 (Conv2D)           (None, 1, 1, 4096)        16781312  \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 1, 1, 4096)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_31 (Conv2D)           (None, 1, 1, 2622)        10742334  \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 2622)              0         \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 2622)              0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 2622)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 512)               1342976   \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 2)                 1026      \n",
            "=================================================================\n",
            "Total params: 146,346,880\n",
            "Trainable params: 1,344,002\n",
            "Non-trainable params: 145,002,878\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Hb8nC8s9YtxN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 6. Training Top Layers on Dataset"
      ]
    },
    {
      "metadata": {
        "id": "2Iem7K2gZER6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "opt = tf.train.AdamOptimizer(learning_rate = 0.001)\n",
        "\n",
        "# We can now compile the model now that layers have been set to non trainable\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qujBG9SxZTxI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Set Hyperparams\n",
        "train_steps = 200\n",
        "val_steps = 50\n",
        "epochs = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7wGjDgyOZHOG",
        "colab_type": "code",
        "outputId": "fd461a8c-28b9-4604-8747-418e11001aac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "# Train the model with validation, shouldn't take too long because of puny dataset\n",
        "history = model.fit( train_dataset, steps_per_epoch = train_steps,\n",
        "                   epochs = epochs,\n",
        "                   validation_data = valid_dataset,\n",
        "                   validation_steps = val_steps)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            " 25/200 [==>...........................] - ETA: 2:01 - loss: 0.6878 - acc: 0.5913"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ObKmb6F0bZo4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "metrics = model.evaluate(valid_dataset,\n",
        "                   steps = val_steps)\n",
        "print(\"model accuracy:\",metrics[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ww7hy0MAbL00",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 7. Adhoc Testing on Single Images"
      ]
    },
    {
      "metadata": {
        "id": "fOEcBQrLbjfD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "image_path = '1000.jpg'\n",
        "\n",
        "from keras.preprocessing import image\n",
        "from keras.applications.imagenet_utils import preprocess_input, decode_predictions\n",
        "import numpy as np\n",
        "from IPython.display import Image\n",
        "\n",
        "#You may replace 1000.jpg with the following [2000.jpg, 3000.jpg, 4000.jpg] or your own uploaded image\n",
        "#For convenience, upload the image into the \"test\" folder to avoid having to restate directory\n",
        "image_check = '1000.jpg'\n",
        "\n",
        "Image(image_check)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q61xmwxgeVZq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "img_path = os.path.join(image_path, image_check)\n",
        "print(img_path)\n",
        "img = image.load_img(img_path, target_size=(224, 224))\n",
        "x = image.img_to_array(img)\n",
        "x = np.expand_dims(x, axis=0)\n",
        "x = preprocess_input(x)\n",
        "print('Input image shape:', x.shape)\n",
        "\n",
        "preds = model.predict(x)\n",
        "print('Predicted:', preds)\n",
        "\n",
        "result= preds[0][0]\n",
        "if result< preds[0][1]:\n",
        "    print(\"Hipster\")\n",
        "else:\n",
        "    print(\"Not Hipster\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D29A0xN6txXD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "It seems the model is able to differentiate between hipsters, at least according to my own perception of what a hipster is."
      ]
    }
  ]
}